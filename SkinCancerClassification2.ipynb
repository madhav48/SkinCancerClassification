{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":71608,"databundleVersionId":7895811,"sourceType":"competition"}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"%matplotlib inline\n\n# python libraties\nimport os, cv2,itertools\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom glob import glob\nfrom PIL import Image\n\n# pytorch libraries\nimport torch\nfrom torch import optim,nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader,Dataset\nfrom torchvision import models,transforms\n\n# sklearn libraries\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\n# ensure results are reproducible\nnp.random.seed(10)\ntorch.manual_seed(10)\ntorch.cuda.manual_seed(10)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T07:08:48.420794Z","iopub.execute_input":"2024-03-15T07:08:48.421230Z","iopub.status.idle":"2024-03-15T07:08:55.887091Z","shell.execute_reply.started":"2024-03-15T07:08:48.421193Z","shell.execute_reply":"2024-03-15T07:08:55.886127Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"data_dir = \"/kaggle/input/aiml-general-championship\"\nprint(os.listdir(data_dir))","metadata":{"execution":{"iopub.status.busy":"2024-03-15T07:08:55.888868Z","iopub.execute_input":"2024-03-15T07:08:55.889309Z","iopub.status.idle":"2024-03-15T07:08:55.895806Z","shell.execute_reply.started":"2024-03-15T07:08:55.889283Z","shell.execute_reply":"2024-03-15T07:08:55.894983Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"['KCDH2024_Training_LesionGroupings.csv', 'KCDH2024_Test_Input', 'sample_submission.csv', 'KCDH2024_Training_Input_10K', 'KCDH2024_Training_GroundTruth.csv']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<h1>Loading Data... </h1>","metadata":{}},{"cell_type":"code","source":"img_data_dir = data_dir + r\"/KCDH2024_Training_Input_10K/KCDH2024_Training_Input_10K\"\nall_image_path = glob(os.path.join(img_data_dir, '*.jpg'))\nimageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x for x in all_image_path}  # key - image fil ename,   value - path to image\nlesion_type_dict = {\n    'nv': 'Melanocytic nevi',\n    'mel': 'dermatofibroma',\n    'bkl': 'Benign keratosis-like lesions ',\n    'bcc': 'Basal cell carcinoma',\n    'akiec': 'Actinic keratoses',\n    'vasc': 'Vascular lesions',\n    'df': 'Dermatofibroma'\n}","metadata":{"execution":{"iopub.status.busy":"2024-03-15T07:08:55.896830Z","iopub.execute_input":"2024-03-15T07:08:55.897124Z","iopub.status.idle":"2024-03-15T07:08:56.261677Z","shell.execute_reply.started":"2024-03-15T07:08:55.897100Z","shell.execute_reply":"2024-03-15T07:08:56.260719Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"len(imageid_path_dict)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T07:08:56.264235Z","iopub.execute_input":"2024-03-15T07:08:56.265049Z","iopub.status.idle":"2024-03-15T07:08:56.270981Z","shell.execute_reply.started":"2024-03-15T07:08:56.265014Z","shell.execute_reply":"2024-03-15T07:08:56.270152Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"9600"},"metadata":{}}]},{"cell_type":"code","source":"img = cv2.imread(imageid_path_dict[\"ISIC_0024308\"])\nimg.shape","metadata":{"execution":{"iopub.status.busy":"2024-03-15T07:08:56.272159Z","iopub.execute_input":"2024-03-15T07:08:56.272416Z","iopub.status.idle":"2024-03-15T07:08:56.319629Z","shell.execute_reply.started":"2024-03-15T07:08:56.272394Z","shell.execute_reply":"2024-03-15T07:08:56.318768Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(450, 600, 3)"},"metadata":{}}]},{"cell_type":"code","source":"def compute_img_mean_std(image_paths):\n    \"\"\"\n        computing the mean and std of three channel on the whole dataset,\n        first we should normalize the image from 0-255 to 0-1\n    \"\"\"\n\n    img_h, img_w = 224, 224              # Size to resize..\n    imgs = []\n    means, stdevs = [], []\n\n    for i in tqdm(range(len(image_paths))):\n        img = cv2.imread(image_paths[i])\n        img = cv2.resize(img, (img_h, img_w))\n        imgs.append(img)\n\n    imgs = np.stack(imgs, axis=3)             \n    print(imgs.shape)\n\n    imgs = imgs.astype(np.float32) / 255.\n\n    for i in range(3):\n        pixels = imgs[:, :, i, :].ravel()  # resize to one row\n        means.append(np.mean(pixels))\n        stdevs.append(np.std(pixels))\n\n    means.reverse()  # BGR --> RGB\n    stdevs.reverse()\n\n    print(\"normMean = {}\".format(means))\n    print(\"normStd = {}\".format(stdevs))\n    return means,stdevs","metadata":{"execution":{"iopub.status.busy":"2024-03-15T07:08:56.320536Z","iopub.execute_input":"2024-03-15T07:08:56.320787Z","iopub.status.idle":"2024-03-15T07:08:56.329426Z","shell.execute_reply.started":"2024-03-15T07:08:56.320764Z","shell.execute_reply":"2024-03-15T07:08:56.328570Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# means , stdevs = compute_img_mean_std(all_image_path)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T07:08:56.330391Z","iopub.execute_input":"2024-03-15T07:08:56.330635Z","iopub.status.idle":"2024-03-15T07:08:56.343056Z","shell.execute_reply.started":"2024-03-15T07:08:56.330614Z","shell.execute_reply":"2024-03-15T07:08:56.342193Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Store Values to save time in future..\nnorm_mean = [0.76696384, 0.54525656, 0.56884694]\nnorm_std = [0.13945772, 0.15192385, 0.16916788]","metadata":{"execution":{"iopub.status.busy":"2024-03-15T07:08:56.344075Z","iopub.execute_input":"2024-03-15T07:08:56.344379Z","iopub.status.idle":"2024-03-15T07:08:56.354365Z","shell.execute_reply.started":"2024-03-15T07:08:56.344355Z","shell.execute_reply":"2024-03-15T07:08:56.353528Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lesion_db = pd.read_csv(os.path.join(data_dir, 'KCDH2024_Training_LesionGroupings.csv'))\ntruth_db = pd.read_csv(os.path.join(data_dir, 'KCDH2024_Training_GroundTruth.csv'))\n\ndf = pd.merge(lesion_db, truth_db, on = 'image', how = 'inner')\ndf.drop(\"diagnosis_confirm_type\", axis = 1, inplace = True)\ndf['path'] = df['image'].map(imageid_path_dict.get)\ndf","metadata":{"execution":{"iopub.status.busy":"2024-03-15T07:08:56.355297Z","iopub.execute_input":"2024-03-15T07:08:56.355535Z","iopub.status.idle":"2024-03-15T07:08:56.446163Z","shell.execute_reply.started":"2024-03-15T07:08:56.355514Z","shell.execute_reply":"2024-03-15T07:08:56.445279Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"              image    lesion_id  MEL  NV  BCC  AKIEC  BKL  DF  VASC  \\\n0      ISIC_0024306  HAM_0000550    0   1    0      0    0   0     0   \n1      ISIC_0024307  HAM_0003577    0   1    0      0    0   0     0   \n2      ISIC_0024308  HAM_0001477    0   1    0      0    0   0     0   \n3      ISIC_0024309  HAM_0000484    0   1    0      0    0   0     0   \n4      ISIC_0024310  HAM_0003350    1   0    0      0    0   0     0   \n...             ...          ...  ...  ..  ...    ...  ...  ..   ...   \n10010  ISIC_0034316  HAM_0004304    1   0    0      0    0   0     0   \n10011  ISIC_0034317  HAM_0006376    1   0    0      0    0   0     0   \n10012  ISIC_0034318  HAM_0000344    0   0    0      0    1   0     0   \n10013  ISIC_0034319  HAM_0000747    0   1    0      0    0   0     0   \n10014  ISIC_0034320  HAM_0002244    0   1    0      0    0   0     0   \n\n                                                    path  \n0      /kaggle/input/aiml-general-championship/KCDH20...  \n1      /kaggle/input/aiml-general-championship/KCDH20...  \n2      /kaggle/input/aiml-general-championship/KCDH20...  \n3      /kaggle/input/aiml-general-championship/KCDH20...  \n4      /kaggle/input/aiml-general-championship/KCDH20...  \n...                                                  ...  \n10010                                               None  \n10011                                               None  \n10012                                               None  \n10013                                               None  \n10014                                               None  \n\n[10015 rows x 10 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>lesion_id</th>\n      <th>MEL</th>\n      <th>NV</th>\n      <th>BCC</th>\n      <th>AKIEC</th>\n      <th>BKL</th>\n      <th>DF</th>\n      <th>VASC</th>\n      <th>path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ISIC_0024306</td>\n      <td>HAM_0000550</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>/kaggle/input/aiml-general-championship/KCDH20...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ISIC_0024307</td>\n      <td>HAM_0003577</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>/kaggle/input/aiml-general-championship/KCDH20...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ISIC_0024308</td>\n      <td>HAM_0001477</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>/kaggle/input/aiml-general-championship/KCDH20...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ISIC_0024309</td>\n      <td>HAM_0000484</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>/kaggle/input/aiml-general-championship/KCDH20...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ISIC_0024310</td>\n      <td>HAM_0003350</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>/kaggle/input/aiml-general-championship/KCDH20...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>10010</th>\n      <td>ISIC_0034316</td>\n      <td>HAM_0004304</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>10011</th>\n      <td>ISIC_0034317</td>\n      <td>HAM_0006376</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>10012</th>\n      <td>ISIC_0034318</td>\n      <td>HAM_0000344</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>10013</th>\n      <td>ISIC_0034319</td>\n      <td>HAM_0000747</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>10014</th>\n      <td>ISIC_0034320</td>\n      <td>HAM_0002244</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>None</td>\n    </tr>\n  </tbody>\n</table>\n<p>10015 rows × 10 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Remove the rows not containg path to images..\ndf = df[df['path'].notna()]\ndf","metadata":{"execution":{"iopub.status.busy":"2024-03-15T07:08:56.451002Z","iopub.execute_input":"2024-03-15T07:08:56.451343Z","iopub.status.idle":"2024-03-15T07:08:56.469047Z","shell.execute_reply.started":"2024-03-15T07:08:56.451320Z","shell.execute_reply":"2024-03-15T07:08:56.468228Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"             image    lesion_id  MEL  NV  BCC  AKIEC  BKL  DF  VASC  \\\n0     ISIC_0024306  HAM_0000550    0   1    0      0    0   0     0   \n1     ISIC_0024307  HAM_0003577    0   1    0      0    0   0     0   \n2     ISIC_0024308  HAM_0001477    0   1    0      0    0   0     0   \n3     ISIC_0024309  HAM_0000484    0   1    0      0    0   0     0   \n4     ISIC_0024310  HAM_0003350    1   0    0      0    0   0     0   \n...            ...          ...  ...  ..  ...    ...  ...  ..   ...   \n9595  ISIC_0033901  HAM_0002342    1   0    0      0    0   0     0   \n9596  ISIC_0033902  HAM_0000048    1   0    0      0    0   0     0   \n9597  ISIC_0033903  HAM_0003367    0   1    0      0    0   0     0   \n9598  ISIC_0033904  HAM_0005820    0   1    0      0    0   0     0   \n9599  ISIC_0033905  HAM_0000042    1   0    0      0    0   0     0   \n\n                                                   path  \n0     /kaggle/input/aiml-general-championship/KCDH20...  \n1     /kaggle/input/aiml-general-championship/KCDH20...  \n2     /kaggle/input/aiml-general-championship/KCDH20...  \n3     /kaggle/input/aiml-general-championship/KCDH20...  \n4     /kaggle/input/aiml-general-championship/KCDH20...  \n...                                                 ...  \n9595  /kaggle/input/aiml-general-championship/KCDH20...  \n9596  /kaggle/input/aiml-general-championship/KCDH20...  \n9597  /kaggle/input/aiml-general-championship/KCDH20...  \n9598  /kaggle/input/aiml-general-championship/KCDH20...  \n9599  /kaggle/input/aiml-general-championship/KCDH20...  \n\n[9600 rows x 10 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>lesion_id</th>\n      <th>MEL</th>\n      <th>NV</th>\n      <th>BCC</th>\n      <th>AKIEC</th>\n      <th>BKL</th>\n      <th>DF</th>\n      <th>VASC</th>\n      <th>path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ISIC_0024306</td>\n      <td>HAM_0000550</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>/kaggle/input/aiml-general-championship/KCDH20...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ISIC_0024307</td>\n      <td>HAM_0003577</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>/kaggle/input/aiml-general-championship/KCDH20...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ISIC_0024308</td>\n      <td>HAM_0001477</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>/kaggle/input/aiml-general-championship/KCDH20...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ISIC_0024309</td>\n      <td>HAM_0000484</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>/kaggle/input/aiml-general-championship/KCDH20...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ISIC_0024310</td>\n      <td>HAM_0003350</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>/kaggle/input/aiml-general-championship/KCDH20...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9595</th>\n      <td>ISIC_0033901</td>\n      <td>HAM_0002342</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>/kaggle/input/aiml-general-championship/KCDH20...</td>\n    </tr>\n    <tr>\n      <th>9596</th>\n      <td>ISIC_0033902</td>\n      <td>HAM_0000048</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>/kaggle/input/aiml-general-championship/KCDH20...</td>\n    </tr>\n    <tr>\n      <th>9597</th>\n      <td>ISIC_0033903</td>\n      <td>HAM_0003367</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>/kaggle/input/aiml-general-championship/KCDH20...</td>\n    </tr>\n    <tr>\n      <th>9598</th>\n      <td>ISIC_0033904</td>\n      <td>HAM_0005820</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>/kaggle/input/aiml-general-championship/KCDH20...</td>\n    </tr>\n    <tr>\n      <th>9599</th>\n      <td>ISIC_0033905</td>\n      <td>HAM_0000042</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>/kaggle/input/aiml-general-championship/KCDH20...</td>\n    </tr>\n  </tbody>\n</table>\n<p>9600 rows × 10 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Convert 7 different columns of different lesisons to single ...\n\ncell_type_idx = []\n\nfor index, row in df.iterrows():\n    cell_type_idx_row = row[\"MEL\"], row[\"NV\"], row[\"BCC\"], row[\"AKIEC\"], row[\"BKL\"], row[\"DF\"], row[\"VASC\"]\n    cell_type_idx.append(cell_type_idx_row.index(1))\n\n# Assign a new column..\ndf[\"cell_type_idx\"] = cell_type_idx   \n\n# Drop older columns..\ndf.drop( columns = [\"MEL\", \"NV\", \"BCC\", \"AKIEC\", \"BKL\", \"DF\", \"VASC\"], inplace = True)\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-15T07:08:56.469914Z","iopub.execute_input":"2024-03-15T07:08:56.470200Z","iopub.status.idle":"2024-03-15T07:08:57.234428Z","shell.execute_reply.started":"2024-03-15T07:08:56.470170Z","shell.execute_reply":"2024-03-15T07:08:57.233469Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/2182716373.py:10: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df[\"cell_type_idx\"] = cell_type_idx\n/tmp/ipykernel_34/2182716373.py:13: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df.drop( columns = [\"MEL\", \"NV\", \"BCC\", \"AKIEC\", \"BKL\", \"DF\", \"VASC\"], inplace = True)\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"          image    lesion_id  \\\n0  ISIC_0024306  HAM_0000550   \n1  ISIC_0024307  HAM_0003577   \n2  ISIC_0024308  HAM_0001477   \n3  ISIC_0024309  HAM_0000484   \n4  ISIC_0024310  HAM_0003350   \n\n                                                path  cell_type_idx  \n0  /kaggle/input/aiml-general-championship/KCDH20...              1  \n1  /kaggle/input/aiml-general-championship/KCDH20...              1  \n2  /kaggle/input/aiml-general-championship/KCDH20...              1  \n3  /kaggle/input/aiml-general-championship/KCDH20...              1  \n4  /kaggle/input/aiml-general-championship/KCDH20...              0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>lesion_id</th>\n      <th>path</th>\n      <th>cell_type_idx</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ISIC_0024306</td>\n      <td>HAM_0000550</td>\n      <td>/kaggle/input/aiml-general-championship/KCDH20...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ISIC_0024307</td>\n      <td>HAM_0003577</td>\n      <td>/kaggle/input/aiml-general-championship/KCDH20...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ISIC_0024308</td>\n      <td>HAM_0001477</td>\n      <td>/kaggle/input/aiml-general-championship/KCDH20...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ISIC_0024309</td>\n      <td>HAM_0000484</td>\n      <td>/kaggle/input/aiml-general-championship/KCDH20...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ISIC_0024310</td>\n      <td>HAM_0003350</td>\n      <td>/kaggle/input/aiml-general-championship/KCDH20...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Determine how many images are associated with each lesion_id ?\ndf_undup = df.groupby('lesion_id').count()\n\n# Filter out lesion_id's that have only one image associated with it\ndf_undup = df_undup[df_undup['image'] == 1]\ndf_undup.reset_index(inplace=True)\ndf_undup.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-15T07:08:57.235761Z","iopub.execute_input":"2024-03-15T07:08:57.236139Z","iopub.status.idle":"2024-03-15T07:08:57.271488Z","shell.execute_reply.started":"2024-03-15T07:08:57.236105Z","shell.execute_reply":"2024-03-15T07:08:57.270666Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"     lesion_id  image  path  cell_type_idx\n0  HAM_0000001      1     1              1\n1  HAM_0000003      1     1              1\n2  HAM_0000004      1     1              1\n3  HAM_0000007      1     1              1\n4  HAM_0000008      1     1              1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lesion_id</th>\n      <th>image</th>\n      <th>path</th>\n      <th>cell_type_idx</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>HAM_0000001</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>HAM_0000003</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>HAM_0000004</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>HAM_0000007</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>HAM_0000008</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_undup = df.groupby('lesion_id').count()\ndf_undup.head()\n\n# Filter out lesion_id's that have only one image associated with it\ndf_undup = df_undup[df_undup['image'] == 1]\ndf_undup.reset_index(inplace=True)\ndf_undup[:]","metadata":{"execution":{"iopub.status.busy":"2024-03-15T07:08:57.272453Z","iopub.execute_input":"2024-03-15T07:08:57.272716Z","iopub.status.idle":"2024-03-15T07:08:57.298343Z","shell.execute_reply.started":"2024-03-15T07:08:57.272692Z","shell.execute_reply":"2024-03-15T07:08:57.297506Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"        lesion_id  image  path  cell_type_idx\n0     HAM_0000001      1     1              1\n1     HAM_0000003      1     1              1\n2     HAM_0000004      1     1              1\n3     HAM_0000007      1     1              1\n4     HAM_0000008      1     1              1\n...           ...    ...   ...            ...\n5592  HAM_0007622      1     1              1\n5593  HAM_0007623      1     1              1\n5594  HAM_0007624      1     1              1\n5595  HAM_0007626      1     1              1\n5596  HAM_0007628      1     1              1\n\n[5597 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lesion_id</th>\n      <th>image</th>\n      <th>path</th>\n      <th>cell_type_idx</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>HAM_0000001</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>HAM_0000003</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>HAM_0000004</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>HAM_0000007</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>HAM_0000008</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5592</th>\n      <td>HAM_0007622</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5593</th>\n      <td>HAM_0007623</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5594</th>\n      <td>HAM_0007624</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5595</th>\n      <td>HAM_0007626</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5596</th>\n      <td>HAM_0007628</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>5597 rows × 4 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Identify lesion_id's that have duplicate images and those that have only one image.\ndef get_duplicates(x):\n    unique_list = list(df_undup['lesion_id'])\n    if x in unique_list:\n        return 'unduplicated'\n    else:\n        return 'duplicated'\n\n# Create a new colum that is a copy of the lesion_id column\ndf['duplicates'] = df['lesion_id']\n# Apply the function to this new column\ndf['duplicates'] = df['duplicates'].apply(get_duplicates)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-15T07:08:57.299370Z","iopub.execute_input":"2024-03-15T07:08:57.299638Z","iopub.status.idle":"2024-03-15T07:09:04.084457Z","shell.execute_reply.started":"2024-03-15T07:08:57.299616Z","shell.execute_reply":"2024-03-15T07:09:04.083532Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/1363690245.py:10: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['duplicates'] = df['lesion_id']\n/tmp/ipykernel_34/1363690245.py:12: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['duplicates'] = df['duplicates'].apply(get_duplicates)\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"          image    lesion_id  \\\n0  ISIC_0024306  HAM_0000550   \n1  ISIC_0024307  HAM_0003577   \n2  ISIC_0024308  HAM_0001477   \n3  ISIC_0024309  HAM_0000484   \n4  ISIC_0024310  HAM_0003350   \n\n                                                path  cell_type_idx  \\\n0  /kaggle/input/aiml-general-championship/KCDH20...              1   \n1  /kaggle/input/aiml-general-championship/KCDH20...              1   \n2  /kaggle/input/aiml-general-championship/KCDH20...              1   \n3  /kaggle/input/aiml-general-championship/KCDH20...              1   \n4  /kaggle/input/aiml-general-championship/KCDH20...              0   \n\n     duplicates  \n0  unduplicated  \n1  unduplicated  \n2  unduplicated  \n3  unduplicated  \n4    duplicated  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>lesion_id</th>\n      <th>path</th>\n      <th>cell_type_idx</th>\n      <th>duplicates</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ISIC_0024306</td>\n      <td>HAM_0000550</td>\n      <td>/kaggle/input/aiml-general-championship/KCDH20...</td>\n      <td>1</td>\n      <td>unduplicated</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ISIC_0024307</td>\n      <td>HAM_0003577</td>\n      <td>/kaggle/input/aiml-general-championship/KCDH20...</td>\n      <td>1</td>\n      <td>unduplicated</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ISIC_0024308</td>\n      <td>HAM_0001477</td>\n      <td>/kaggle/input/aiml-general-championship/KCDH20...</td>\n      <td>1</td>\n      <td>unduplicated</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ISIC_0024309</td>\n      <td>HAM_0000484</td>\n      <td>/kaggle/input/aiml-general-championship/KCDH20...</td>\n      <td>1</td>\n      <td>unduplicated</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ISIC_0024310</td>\n      <td>HAM_0003350</td>\n      <td>/kaggle/input/aiml-general-championship/KCDH20...</td>\n      <td>0</td>\n      <td>duplicated</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df['duplicates'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-03-15T07:09:04.085870Z","iopub.execute_input":"2024-03-15T07:09:04.086329Z","iopub.status.idle":"2024-03-15T07:09:04.097603Z","shell.execute_reply.started":"2024-03-15T07:09:04.086298Z","shell.execute_reply":"2024-03-15T07:09:04.096693Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"duplicates\nunduplicated    5597\nduplicated      4003\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# Filter out images that don't have duplicates (count = 1)  (Removed augmented images)\n# We will use this data to get validation set..\ndf_undup = df[df['duplicates'] == 'unduplicated']\ndf_undup.shape","metadata":{"execution":{"iopub.status.busy":"2024-03-15T07:09:04.098752Z","iopub.execute_input":"2024-03-15T07:09:04.099055Z","iopub.status.idle":"2024-03-15T07:09:04.113427Z","shell.execute_reply.started":"2024-03-15T07:09:04.099032Z","shell.execute_reply":"2024-03-15T07:09:04.112454Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"(5597, 5)"},"metadata":{}}]},{"cell_type":"code","source":"# Create a val set using df as none of these images have augmented duplicates in the training set now..\ny = df_undup['cell_type_idx']\n_, df_val = train_test_split(df_undup, test_size=0.2, random_state=101, stratify=y)\ndf_val.shape","metadata":{"execution":{"iopub.status.busy":"2024-03-15T07:09:04.114713Z","iopub.execute_input":"2024-03-15T07:09:04.115038Z","iopub.status.idle":"2024-03-15T07:09:04.131371Z","shell.execute_reply.started":"2024-03-15T07:09:04.115014Z","shell.execute_reply":"2024-03-15T07:09:04.130572Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"(1120, 5)"},"metadata":{}}]},{"cell_type":"code","source":"df_val['cell_type_idx'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-03-15T07:09:04.132345Z","iopub.execute_input":"2024-03-15T07:09:04.132599Z","iopub.status.idle":"2024-03-15T07:09:04.141894Z","shell.execute_reply.started":"2024-03-15T07:09:04.132577Z","shell.execute_reply":"2024-03-15T07:09:04.141004Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"cell_type_idx\n1    891\n4     89\n0     52\n2     37\n3     30\n6     13\n5      8\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# Remove the validation rows from the original data to get training rows..\n\n# This function identifies if an image is part of the train or val set.\ndef get_val_rows(x):\n    # create a list of all the lesion_id's in the val set\n    val_list = list(df_val['image'])\n    if str(x) in val_list:\n        return 'val'\n    else:\n        return 'train'\n\n# Identify train and val rows..\n# Create a new colum that is a copy of the image column\ndf['train_or_val'] = df['image']\n\n# Apply the function to this new column\ndf['train_or_val'] = df['train_or_val'].apply(get_val_rows)\n\n# Filter out training rows\ndf_train = df[df['train_or_val'] == 'train']\ndf_train = df_train.drop('train_or_val', axis = 1, inplace = False)\nprint(len(df_train))\nprint(len(df_val))","metadata":{"execution":{"iopub.status.busy":"2024-03-15T07:09:04.143149Z","iopub.execute_input":"2024-03-15T07:09:04.143546Z","iopub.status.idle":"2024-03-15T07:09:05.742758Z","shell.execute_reply.started":"2024-03-15T07:09:04.143517Z","shell.execute_reply":"2024-03-15T07:09:05.741813Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/2742417337.py:14: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['train_or_val'] = df['image']\n","output_type":"stream"},{"name":"stdout","text":"8480\n1120\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_34/2742417337.py:17: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['train_or_val'] = df['train_or_val'].apply(get_val_rows)\n","output_type":"stream"}]},{"cell_type":"code","source":"df_train['cell_type_idx'].value_counts()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-03-15T07:09:05.744369Z","iopub.execute_input":"2024-03-15T07:09:05.744685Z","iopub.status.idle":"2024-03-15T07:09:05.751869Z","shell.execute_reply.started":"2024-03-15T07:09:05.744658Z","shell.execute_reply":"2024-03-15T07:09:05.750880Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"cell_type_idx\n1    5566\n4     966\n0     963\n2     458\n3     297\n6     125\n5     105\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# Duplicate rows to balance the number of rows in 7 classes..\ndata_aug_rate = [5,0,10,15,5,50,40]\n\n# Iterate over unique values of 'cell_type_idx'\nfor i in df_train['cell_type_idx'].unique():\n\n    if data_aug_rate[i]:\n        \n        # Filter the DataFrame for the current value of 'cell_type_idx'\n        filtered_df = df_train[df_train['cell_type_idx'] == i]\n        \n        # Duplicate rows based on the data augmentation rate for this value of 'cell_type_idx'\n        duplicated_rows = filtered_df.sample(frac=data_aug_rate[i] - 1, replace=True)\n        \n        # Concatenate the original DataFrame with the duplicated rows\n        df_train = pd.concat([df_train, duplicated_rows], ignore_index=True)\n\ndf_train['cell_type_idx'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-03-15T07:09:05.753135Z","iopub.execute_input":"2024-03-15T07:09:05.753735Z","iopub.status.idle":"2024-03-15T07:09:05.782346Z","shell.execute_reply.started":"2024-03-15T07:09:05.753709Z","shell.execute_reply":"2024-03-15T07:09:05.781575Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"cell_type_idx\n1    5566\n5    5250\n6    5000\n4    4830\n0    4815\n2    4580\n3    4455\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"<h1>DataSet Spliiting</h1>","metadata":{}},{"cell_type":"code","source":"# Split the test set again in a validation set and a true test set:\n\ndf_val, df_test = train_test_split(df_val, test_size=0.5)\ndf_train = df_train.reset_index()\ndf_val = df_val.reset_index()\ndf_test = df_test.reset_index()","metadata":{"execution":{"iopub.status.busy":"2024-03-15T07:09:05.783303Z","iopub.execute_input":"2024-03-15T07:09:05.783554Z","iopub.status.idle":"2024-03-15T07:09:05.797546Z","shell.execute_reply.started":"2024-03-15T07:09:05.783533Z","shell.execute_reply":"2024-03-15T07:09:05.796734Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"print(len(df_test))\ndf_test['cell_type_idx'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-03-15T07:09:05.798630Z","iopub.execute_input":"2024-03-15T07:09:05.799002Z","iopub.status.idle":"2024-03-15T07:09:05.808245Z","shell.execute_reply.started":"2024-03-15T07:09:05.798943Z","shell.execute_reply":"2024-03-15T07:09:05.807287Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"560\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"cell_type_idx\n1    440\n4     53\n0     24\n2     19\n3     17\n6      4\n5      3\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"<h1>Define Model</h1>","metadata":{}},{"cell_type":"code","source":"# feature_extract is a boolean that defines finetuning or feature extracting. \n# If feature_extract = False, the model is finetuned and all model parameters are updated. \n# If feature_extract = True, only the last layer parameters are updated, the others remain fixed.\n\ndef set_parameter_requires_grad(model, feature_extracting):\n    if feature_extracting:\n        for param in model.parameters():\n            param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2024-03-15T07:09:05.809536Z","iopub.execute_input":"2024-03-15T07:09:05.809813Z","iopub.status.idle":"2024-03-15T07:09:05.817892Z","shell.execute_reply.started":"2024-03-15T07:09:05.809789Z","shell.execute_reply":"2024-03-15T07:09:05.817153Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n    # Initialize these variables which will be set\n    model_ft = None\n    input_size = 0\n\n    if model_name == \"resnet\":\n        \"\"\" Resnet18, resnet34, resnet50, resnet101\n        \"\"\"\n        model_ft = models.resnet50(pretrained=True)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.fc.in_features\n        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n        input_size = 224\n\n    elif model_name == \"mobilenet\":\n        model_ft = models.mobilenet_v2(pretrained=True, progress=True)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        input_size = 224\n\n    else:\n        print(\"Invalid model name, exiting...\")\n        exit()\n\n    return model_ft, input_size","metadata":{"execution":{"iopub.status.busy":"2024-03-15T07:09:05.819089Z","iopub.execute_input":"2024-03-15T07:09:05.819396Z","iopub.status.idle":"2024-03-15T07:09:05.832966Z","shell.execute_reply.started":"2024-03-15T07:09:05.819372Z","shell.execute_reply":"2024-03-15T07:09:05.832188Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# Define a new model variable..\n\nmodel_name = \"mobilenet\"\nnum_classes = 7\nfeature_extract = False\n# Initialize the model\nmodel_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n\n# Define the device:\ndevice = torch.device('cuda:0')\n# device = torch.device('cpu') # If using cpu\n# Put the model on the device:\nmodel = model_ft.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T07:09:05.834031Z","iopub.execute_input":"2024-03-15T07:09:05.834346Z","iopub.status.idle":"2024-03-15T07:09:06.436370Z","shell.execute_reply.started":"2024-03-15T07:09:05.834322Z","shell.execute_reply":"2024-03-15T07:09:06.435525Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n100%|██████████| 13.6M/13.6M [00:00<00:00, 112MB/s] \n","output_type":"stream"}]},{"cell_type":"markdown","source":"<H1>Augmenting Data...</H1>","metadata":{}},{"cell_type":"code","source":"# define the transformation of the train images.\ntrain_transform = transforms.Compose([transforms.Resize((input_size,input_size)),transforms.RandomHorizontalFlip(),\n                                      transforms.RandomVerticalFlip(),\n                                      transforms.RandomRotation(20),\n                                      transforms.ColorJitter(brightness=0.1, contrast=0.1, hue=0.1),\n                                      transforms.ToTensor(), \n                                      transforms.Normalize(norm_mean, norm_std)])\n\n# define the transformation of the val images.\nval_transform = transforms.Compose([transforms.Resize((input_size,input_size)), \n                                    transforms.ToTensor(),\n                                    transforms.Normalize(norm_mean, norm_std)])\n\n# define the transformation of the test images.\ntest_transform = transforms.Compose([transforms.Resize((input_size,input_size)), \n                                     transforms.ToTensor(),\n                                    transforms.Normalize(norm_mean, norm_std)])","metadata":{"execution":{"iopub.status.busy":"2024-03-15T07:09:06.437593Z","iopub.execute_input":"2024-03-15T07:09:06.437976Z","iopub.status.idle":"2024-03-15T07:09:06.445586Z","shell.execute_reply.started":"2024-03-15T07:09:06.437931Z","shell.execute_reply":"2024-03-15T07:09:06.444699Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# Define a pytorch dataloader for dataset..\nclass HAM10000(Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        # Load data and get label\n        X = Image.open(self.df['path'][index])\n        y = torch.tensor(int(self.df['cell_type_idx'][index]))\n\n        if self.transform:\n            X = self.transform(X)\n\n        return X, y","metadata":{"execution":{"iopub.status.busy":"2024-03-15T07:09:06.451652Z","iopub.execute_input":"2024-03-15T07:09:06.451936Z","iopub.status.idle":"2024-03-15T07:09:06.461110Z","shell.execute_reply.started":"2024-03-15T07:09:06.451911Z","shell.execute_reply":"2024-03-15T07:09:06.460276Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# Define the training set using the table train_df and using the defined transitions (train_transform)\ntraining_set = HAM10000(df_train, transform=train_transform)\ntrain_loader = DataLoader(training_set, batch_size=32, shuffle=True, num_workers=0)\n\n# Same for the validation set:\nvalidation_set = HAM10000(df_val, transform=train_transform)\nval_loader = DataLoader(validation_set, batch_size=32, shuffle=False, num_workers=0)\n\n# Same for the test set:\ntest_set = HAM10000(df_test, transform=train_transform)\ntest_loader = DataLoader(test_set, batch_size=32, shuffle=False, num_workers=0)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T07:09:06.462105Z","iopub.execute_input":"2024-03-15T07:09:06.462371Z","iopub.status.idle":"2024-03-15T07:09:06.472228Z","shell.execute_reply.started":"2024-03-15T07:09:06.462348Z","shell.execute_reply":"2024-03-15T07:09:06.471414Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# set optimizer and loss function\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss().to(device)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T07:09:06.473417Z","iopub.execute_input":"2024-03-15T07:09:06.473699Z","iopub.status.idle":"2024-03-15T07:09:06.487849Z","shell.execute_reply.started":"2024-03-15T07:09:06.473675Z","shell.execute_reply":"2024-03-15T07:09:06.486994Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# this function is used during training process, to calculate the loss and accuracy\nclass AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"execution":{"iopub.status.busy":"2024-03-15T07:09:06.489257Z","iopub.execute_input":"2024-03-15T07:09:06.489534Z","iopub.status.idle":"2024-03-15T07:09:06.498089Z","shell.execute_reply.started":"2024-03-15T07:09:06.489511Z","shell.execute_reply":"2024-03-15T07:09:06.497300Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"total_loss_train, total_acc_train = [],[]\n\ndef train(train_loader, model, criterion, optimizer, epoch):\n    model.train()\n    train_loss = AverageMeter()\n    train_acc = AverageMeter()\n    curr_iter = (epoch - 1) * len(train_loader)\n    for i, data in enumerate(train_loader):\n        \n        images, labels = data\n        N = images.size(0)\n        images = Variable(images).to(device)\n        labels = Variable(labels).to(device)\n\n        optimizer.zero_grad()\n        outputs = model(images)\n\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        prediction = outputs.max(1, keepdim=True)[1]\n        train_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)\n        train_loss.update(loss.item())\n        curr_iter += 1\n        if (i + 1) % 100 == 0:\n            print('[epoch %d], [iter %d / %d], [train loss %.5f], [train acc %.5f]' % (\n                epoch, i + 1, len(train_loader), train_loss.avg, train_acc.avg))\n            total_loss_train.append(train_loss.avg)\n            total_acc_train.append(train_acc.avg)\n    return train_loss.avg, train_acc.avg","metadata":{"execution":{"iopub.status.busy":"2024-03-15T07:09:06.499039Z","iopub.execute_input":"2024-03-15T07:09:06.499306Z","iopub.status.idle":"2024-03-15T07:09:06.511430Z","shell.execute_reply.started":"2024-03-15T07:09:06.499284Z","shell.execute_reply":"2024-03-15T07:09:06.510653Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"def validate(val_loader, model, criterion, optimizer, epoch):\n    model.eval()\n    val_loss = AverageMeter()\n    val_acc = AverageMeter()\n    \n    with torch.no_grad():\n        for i, data in tqdm(enumerate(val_loader)):\n            images, labels = data\n            N = images.size(0)\n            images = Variable(images).to(device)\n            labels = Variable(labels).to(device)\n\n            outputs = model(images)\n            prediction = outputs.max(1, keepdim=True)[1]\n\n            val_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)\n            val_loss.update(criterion(outputs, labels).item())\n\n    print('------------------------------------------------------------')\n    print('[epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, val_loss.avg, val_acc.avg))\n    print('------------------------------------------------------------')\n    return val_loss.avg, val_acc.avg","metadata":{"execution":{"iopub.status.busy":"2024-03-15T07:09:06.512388Z","iopub.execute_input":"2024-03-15T07:09:06.512651Z","iopub.status.idle":"2024-03-15T07:09:06.525297Z","shell.execute_reply.started":"2024-03-15T07:09:06.512628Z","shell.execute_reply":"2024-03-15T07:09:06.524506Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# Start Training..\n\nepoch_num = 10\nbest_val_acc = 0\ntotal_loss_val, total_acc_val = [],[]\nfor epoch in tqdm(range(1, epoch_num+1)):\n    loss_train, acc_train = train(train_loader, model, criterion, optimizer, epoch)\n    loss_val, acc_val = validate(val_loader, model, criterion, optimizer, epoch)\n    total_loss_val.append(loss_val)\n    total_acc_val.append(acc_val)\n    if acc_val > best_val_acc:\n        best_val_acc = acc_val\n        print('*****************************************************')\n        print('best record: [epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, loss_val, acc_val))\n        print('*****************************************************')","metadata":{"execution":{"iopub.status.busy":"2024-03-15T07:09:57.718004Z","iopub.execute_input":"2024-03-15T07:09:57.718370Z","iopub.status.idle":"2024-03-15T08:46:23.678105Z","shell.execute_reply.started":"2024-03-15T07:09:57.718342Z","shell.execute_reply":"2024-03-15T08:46:23.677145Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stderr","text":"  0%|          | 0/10 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"[epoch 1], [iter 100 / 1078], [train loss 1.60715], [train acc 0.54312]\n[epoch 1], [iter 200 / 1078], [train loss 1.27424], [train acc 0.59516]\n[epoch 1], [iter 300 / 1078], [train loss 1.12017], [train acc 0.62979]\n[epoch 1], [iter 400 / 1078], [train loss 1.03159], [train acc 0.65312]\n[epoch 1], [iter 500 / 1078], [train loss 0.97033], [train acc 0.66950]\n[epoch 1], [iter 600 / 1078], [train loss 0.92006], [train acc 0.68370]\n[epoch 1], [iter 700 / 1078], [train loss 0.88075], [train acc 0.69634]\n[epoch 1], [iter 800 / 1078], [train loss 0.84895], [train acc 0.70668]\n[epoch 1], [iter 900 / 1078], [train loss 0.82254], [train acc 0.71424]\n[epoch 1], [iter 1000 / 1078], [train loss 0.79945], [train acc 0.72062]\n","output_type":"stream"},{"name":"stderr","text":"\n0it [00:00, ?it/s]\u001b[A\n1it [00:01,  1.08s/it]\u001b[A\n2it [00:01,  1.03it/s]\u001b[A\n3it [00:02,  1.08it/s]\u001b[A\n4it [00:03,  1.11it/s]\u001b[A\n5it [00:04,  1.13it/s]\u001b[A\n6it [00:05,  1.15it/s]\u001b[A\n7it [00:06,  1.15it/s]\u001b[A\n8it [00:07,  1.18it/s]\u001b[A\n9it [00:07,  1.17it/s]\u001b[A\n10it [00:08,  1.14it/s]\u001b[A\n11it [00:09,  1.18it/s]\u001b[A\n12it [00:10,  1.22it/s]\u001b[A\n13it [00:11,  1.21it/s]\u001b[A\n14it [00:12,  1.18it/s]\u001b[A\n15it [00:12,  1.18it/s]\u001b[A\n16it [00:13,  1.20it/s]\u001b[A\n17it [00:14,  1.17it/s]\u001b[A\n18it [00:15,  1.19it/s]\u001b[A\n 10%|█         | 1/10 [11:19<1:41:53, 679.22s/it]","output_type":"stream"},{"name":"stdout","text":"------------------------------------------------------------\n[epoch 1], [val loss 0.61614], [val acc 0.79514]\n------------------------------------------------------------\n*****************************************************\nbest record: [epoch 1], [val loss 0.61614], [val acc 0.79514]\n*****************************************************\n[epoch 2], [iter 100 / 1078], [train loss 0.53757], [train acc 0.79906]\n[epoch 2], [iter 200 / 1078], [train loss 0.54916], [train acc 0.79641]\n[epoch 2], [iter 300 / 1078], [train loss 0.54587], [train acc 0.79656]\n[epoch 2], [iter 400 / 1078], [train loss 0.53761], [train acc 0.79922]\n[epoch 2], [iter 500 / 1078], [train loss 0.53634], [train acc 0.79794]\n[epoch 2], [iter 600 / 1078], [train loss 0.53337], [train acc 0.80036]\n[epoch 2], [iter 700 / 1078], [train loss 0.52737], [train acc 0.80192]\n[epoch 2], [iter 800 / 1078], [train loss 0.52617], [train acc 0.80312]\n[epoch 2], [iter 900 / 1078], [train loss 0.52344], [train acc 0.80299]\n[epoch 2], [iter 1000 / 1078], [train loss 0.52113], [train acc 0.80437]\n","output_type":"stream"},{"name":"stderr","text":"\n0it [00:00, ?it/s]\u001b[A\n1it [00:00,  2.18it/s]\u001b[A\n2it [00:00,  2.17it/s]\u001b[A\n3it [00:01,  2.21it/s]\u001b[A\n4it [00:01,  2.20it/s]\u001b[A\n5it [00:02,  2.18it/s]\u001b[A\n6it [00:02,  2.17it/s]\u001b[A\n7it [00:03,  2.17it/s]\u001b[A\n8it [00:03,  2.18it/s]\u001b[A\n9it [00:04,  2.18it/s]\u001b[A\n10it [00:04,  2.19it/s]\u001b[A\n11it [00:05,  2.20it/s]\u001b[A\n12it [00:05,  2.17it/s]\u001b[A\n13it [00:05,  2.19it/s]\u001b[A\n14it [00:06,  2.18it/s]\u001b[A\n15it [00:06,  2.15it/s]\u001b[A\n16it [00:07,  2.16it/s]\u001b[A\n17it [00:07,  2.16it/s]\u001b[A\n18it [00:08,  2.24it/s]\u001b[A\n 20%|██        | 2/10 [20:44<1:21:38, 612.31s/it]","output_type":"stream"},{"name":"stdout","text":"------------------------------------------------------------\n[epoch 2], [val loss 0.34120], [val acc 0.88194]\n------------------------------------------------------------\n*****************************************************\nbest record: [epoch 2], [val loss 0.34120], [val acc 0.88194]\n*****************************************************\n[epoch 3], [iter 100 / 1078], [train loss 0.46610], [train acc 0.83062]\n[epoch 3], [iter 200 / 1078], [train loss 0.46130], [train acc 0.82875]\n[epoch 3], [iter 300 / 1078], [train loss 0.46440], [train acc 0.82740]\n[epoch 3], [iter 400 / 1078], [train loss 0.45878], [train acc 0.82891]\n[epoch 3], [iter 500 / 1078], [train loss 0.45678], [train acc 0.83000]\n[epoch 3], [iter 600 / 1078], [train loss 0.45621], [train acc 0.83130]\n[epoch 3], [iter 700 / 1078], [train loss 0.45122], [train acc 0.83205]\n[epoch 3], [iter 800 / 1078], [train loss 0.44707], [train acc 0.83375]\n[epoch 3], [iter 900 / 1078], [train loss 0.44336], [train acc 0.83514]\n[epoch 3], [iter 1000 / 1078], [train loss 0.44292], [train acc 0.83506]\n","output_type":"stream"},{"name":"stderr","text":"\n0it [00:00, ?it/s]\u001b[A\n1it [00:00,  2.18it/s]\u001b[A\n2it [00:00,  2.07it/s]\u001b[A\n3it [00:01,  2.11it/s]\u001b[A\n4it [00:01,  2.12it/s]\u001b[A\n5it [00:02,  2.14it/s]\u001b[A\n6it [00:02,  2.15it/s]\u001b[A\n7it [00:03,  2.15it/s]\u001b[A\n8it [00:03,  2.15it/s]\u001b[A\n9it [00:04,  2.17it/s]\u001b[A\n10it [00:04,  2.18it/s]\u001b[A\n11it [00:05,  2.18it/s]\u001b[A\n12it [00:05,  2.16it/s]\u001b[A\n13it [00:06,  2.17it/s]\u001b[A\n14it [00:06,  2.14it/s]\u001b[A\n15it [00:06,  2.13it/s]\u001b[A\n16it [00:07,  2.14it/s]\u001b[A\n17it [00:07,  2.16it/s]\u001b[A\n18it [00:08,  2.21it/s]\u001b[A\n 30%|███       | 3/10 [30:14<1:09:10, 592.95s/it]","output_type":"stream"},{"name":"stdout","text":"------------------------------------------------------------\n[epoch 3], [val loss 0.35364], [val acc 0.87674]\n------------------------------------------------------------\n[epoch 4], [iter 100 / 1078], [train loss 0.40245], [train acc 0.85250]\n[epoch 4], [iter 200 / 1078], [train loss 0.40440], [train acc 0.85328]\n[epoch 4], [iter 300 / 1078], [train loss 0.39739], [train acc 0.85406]\n[epoch 4], [iter 400 / 1078], [train loss 0.39975], [train acc 0.85156]\n[epoch 4], [iter 500 / 1078], [train loss 0.39341], [train acc 0.85344]\n[epoch 4], [iter 600 / 1078], [train loss 0.39179], [train acc 0.85427]\n[epoch 4], [iter 700 / 1078], [train loss 0.38968], [train acc 0.85473]\n[epoch 4], [iter 800 / 1078], [train loss 0.39536], [train acc 0.85289]\n[epoch 4], [iter 900 / 1078], [train loss 0.39631], [train acc 0.85288]\n[epoch 4], [iter 1000 / 1078], [train loss 0.39349], [train acc 0.85366]\n","output_type":"stream"},{"name":"stderr","text":"\n0it [00:00, ?it/s]\u001b[A\n1it [00:00,  2.18it/s]\u001b[A\n2it [00:00,  2.19it/s]\u001b[A\n3it [00:01,  2.14it/s]\u001b[A\n4it [00:01,  2.14it/s]\u001b[A\n5it [00:02,  2.16it/s]\u001b[A\n6it [00:02,  2.17it/s]\u001b[A\n7it [00:03,  2.13it/s]\u001b[A\n8it [00:03,  2.15it/s]\u001b[A\n9it [00:04,  2.16it/s]\u001b[A\n10it [00:04,  2.17it/s]\u001b[A\n11it [00:05,  2.19it/s]\u001b[A\n12it [00:05,  2.19it/s]\u001b[A\n13it [00:05,  2.21it/s]\u001b[A\n14it [00:06,  2.19it/s]\u001b[A\n15it [00:06,  2.17it/s]\u001b[A\n16it [00:07,  2.15it/s]\u001b[A\n17it [00:07,  2.17it/s]\u001b[A\n18it [00:08,  2.23it/s]\u001b[A\n 40%|████      | 4/10 [39:40<58:14, 582.35s/it]  ","output_type":"stream"},{"name":"stdout","text":"------------------------------------------------------------\n[epoch 4], [val loss 0.47600], [val acc 0.86979]\n------------------------------------------------------------\n[epoch 5], [iter 100 / 1078], [train loss 0.34830], [train acc 0.86906]\n[epoch 5], [iter 200 / 1078], [train loss 0.35827], [train acc 0.86734]\n[epoch 5], [iter 300 / 1078], [train loss 0.36636], [train acc 0.86260]\n[epoch 5], [iter 400 / 1078], [train loss 0.36336], [train acc 0.86461]\n[epoch 5], [iter 500 / 1078], [train loss 0.35963], [train acc 0.86462]\n[epoch 5], [iter 600 / 1078], [train loss 0.35972], [train acc 0.86438]\n[epoch 5], [iter 700 / 1078], [train loss 0.35955], [train acc 0.86442]\n[epoch 5], [iter 800 / 1078], [train loss 0.36006], [train acc 0.86445]\n[epoch 5], [iter 900 / 1078], [train loss 0.36142], [train acc 0.86431]\n[epoch 5], [iter 1000 / 1078], [train loss 0.36045], [train acc 0.86472]\n","output_type":"stream"},{"name":"stderr","text":"\n0it [00:00, ?it/s]\u001b[A\n1it [00:00,  2.08it/s]\u001b[A\n2it [00:00,  2.12it/s]\u001b[A\n3it [00:01,  2.14it/s]\u001b[A\n4it [00:01,  2.16it/s]\u001b[A\n5it [00:02,  2.16it/s]\u001b[A\n6it [00:02,  2.14it/s]\u001b[A\n7it [00:03,  2.06it/s]\u001b[A\n8it [00:03,  2.09it/s]\u001b[A\n9it [00:04,  2.11it/s]\u001b[A\n10it [00:04,  2.10it/s]\u001b[A\n11it [00:05,  2.11it/s]\u001b[A\n12it [00:05,  2.12it/s]\u001b[A\n13it [00:06,  2.04it/s]\u001b[A\n14it [00:06,  2.01it/s]\u001b[A\n15it [00:07,  2.01it/s]\u001b[A\n16it [00:07,  2.05it/s]\u001b[A\n17it [00:08,  2.08it/s]\u001b[A\n18it [00:08,  2.15it/s]\u001b[A\n 50%|█████     | 5/10 [49:07<48:04, 576.85s/it]","output_type":"stream"},{"name":"stdout","text":"------------------------------------------------------------\n[epoch 5], [val loss 0.44566], [val acc 0.84028]\n------------------------------------------------------------\n[epoch 6], [iter 100 / 1078], [train loss 0.31038], [train acc 0.88375]\n[epoch 6], [iter 200 / 1078], [train loss 0.32830], [train acc 0.87516]\n[epoch 6], [iter 300 / 1078], [train loss 0.32067], [train acc 0.87833]\n[epoch 6], [iter 400 / 1078], [train loss 0.31715], [train acc 0.87914]\n[epoch 6], [iter 500 / 1078], [train loss 0.31574], [train acc 0.87944]\n[epoch 6], [iter 600 / 1078], [train loss 0.32174], [train acc 0.87807]\n[epoch 6], [iter 700 / 1078], [train loss 0.32247], [train acc 0.87879]\n[epoch 6], [iter 800 / 1078], [train loss 0.32350], [train acc 0.87750]\n[epoch 6], [iter 900 / 1078], [train loss 0.32511], [train acc 0.87733]\n[epoch 6], [iter 1000 / 1078], [train loss 0.32229], [train acc 0.87869]\n","output_type":"stream"},{"name":"stderr","text":"\n0it [00:00, ?it/s]\u001b[A\n1it [00:00,  1.85it/s]\u001b[A\n2it [00:01,  2.01it/s]\u001b[A\n3it [00:01,  2.08it/s]\u001b[A\n4it [00:01,  2.07it/s]\u001b[A\n5it [00:02,  2.11it/s]\u001b[A\n6it [00:02,  2.12it/s]\u001b[A\n7it [00:03,  2.07it/s]\u001b[A\n8it [00:03,  2.07it/s]\u001b[A\n9it [00:04,  2.07it/s]\u001b[A\n10it [00:04,  2.07it/s]\u001b[A\n11it [00:05,  2.07it/s]\u001b[A\n12it [00:05,  2.03it/s]\u001b[A\n13it [00:06,  2.06it/s]\u001b[A\n14it [00:06,  2.08it/s]\u001b[A\n15it [00:07,  2.08it/s]\u001b[A\n16it [00:07,  2.08it/s]\u001b[A\n17it [00:08,  2.10it/s]\u001b[A\n18it [00:08,  2.13it/s]\u001b[A\n 60%|██████    | 6/10 [58:40<38:22, 575.53s/it]","output_type":"stream"},{"name":"stdout","text":"------------------------------------------------------------\n[epoch 6], [val loss 0.41751], [val acc 0.83681]\n------------------------------------------------------------\n[epoch 7], [iter 100 / 1078], [train loss 0.27833], [train acc 0.89656]\n[epoch 7], [iter 200 / 1078], [train loss 0.30128], [train acc 0.88891]\n[epoch 7], [iter 300 / 1078], [train loss 0.31464], [train acc 0.88240]\n[epoch 7], [iter 400 / 1078], [train loss 0.31341], [train acc 0.88109]\n[epoch 7], [iter 500 / 1078], [train loss 0.31071], [train acc 0.88319]\n[epoch 7], [iter 600 / 1078], [train loss 0.30896], [train acc 0.88469]\n[epoch 7], [iter 700 / 1078], [train loss 0.30735], [train acc 0.88500]\n[epoch 7], [iter 800 / 1078], [train loss 0.30686], [train acc 0.88543]\n[epoch 7], [iter 900 / 1078], [train loss 0.30514], [train acc 0.88601]\n[epoch 7], [iter 1000 / 1078], [train loss 0.30301], [train acc 0.88719]\n","output_type":"stream"},{"name":"stderr","text":"\n0it [00:00, ?it/s]\u001b[A\n1it [00:00,  2.13it/s]\u001b[A\n2it [00:00,  2.16it/s]\u001b[A\n3it [00:01,  2.14it/s]\u001b[A\n4it [00:01,  2.14it/s]\u001b[A\n5it [00:02,  2.14it/s]\u001b[A\n6it [00:02,  2.14it/s]\u001b[A\n7it [00:03,  2.11it/s]\u001b[A\n8it [00:03,  2.12it/s]\u001b[A\n9it [00:04,  2.11it/s]\u001b[A\n10it [00:04,  2.07it/s]\u001b[A\n11it [00:05,  2.03it/s]\u001b[A\n12it [00:05,  1.98it/s]\u001b[A\n13it [00:06,  1.99it/s]\u001b[A\n14it [00:06,  2.01it/s]\u001b[A\n15it [00:07,  2.03it/s]\u001b[A\n16it [00:07,  2.01it/s]\u001b[A\n17it [00:08,  2.03it/s]\u001b[A\n18it [00:08,  2.12it/s]\u001b[A\n 70%|███████   | 7/10 [1:08:08<28:38, 572.84s/it]","output_type":"stream"},{"name":"stdout","text":"------------------------------------------------------------\n[epoch 7], [val loss 0.30919], [val acc 0.87674]\n------------------------------------------------------------\n[epoch 8], [iter 100 / 1078], [train loss 0.26217], [train acc 0.89844]\n[epoch 8], [iter 200 / 1078], [train loss 0.28432], [train acc 0.89312]\n[epoch 8], [iter 300 / 1078], [train loss 0.28086], [train acc 0.89479]\n[epoch 8], [iter 400 / 1078], [train loss 0.27655], [train acc 0.89758]\n[epoch 8], [iter 500 / 1078], [train loss 0.27935], [train acc 0.89600]\n[epoch 8], [iter 600 / 1078], [train loss 0.27744], [train acc 0.89667]\n[epoch 8], [iter 700 / 1078], [train loss 0.28196], [train acc 0.89522]\n[epoch 8], [iter 800 / 1078], [train loss 0.29871], [train acc 0.88895]\n[epoch 8], [iter 900 / 1078], [train loss 0.30142], [train acc 0.88795]\n[epoch 8], [iter 1000 / 1078], [train loss 0.29834], [train acc 0.88881]\n","output_type":"stream"},{"name":"stderr","text":"\n0it [00:00, ?it/s]\u001b[A\n1it [00:00,  1.97it/s]\u001b[A\n2it [00:00,  2.03it/s]\u001b[A\n3it [00:01,  2.05it/s]\u001b[A\n4it [00:01,  2.04it/s]\u001b[A\n5it [00:02,  2.07it/s]\u001b[A\n6it [00:02,  2.07it/s]\u001b[A\n7it [00:03,  2.06it/s]\u001b[A\n8it [00:03,  2.04it/s]\u001b[A\n9it [00:04,  2.03it/s]\u001b[A\n10it [00:04,  2.02it/s]\u001b[A\n11it [00:05,  2.05it/s]\u001b[A\n12it [00:05,  2.03it/s]\u001b[A\n13it [00:06,  1.99it/s]\u001b[A\n14it [00:06,  1.98it/s]\u001b[A\n15it [00:07,  2.00it/s]\u001b[A\n16it [00:07,  2.00it/s]\u001b[A\n17it [00:08,  1.95it/s]\u001b[A\n18it [00:08,  2.07it/s]\u001b[A\n 80%|████████  | 8/10 [1:17:34<19:01, 570.66s/it]","output_type":"stream"},{"name":"stdout","text":"------------------------------------------------------------\n[epoch 8], [val loss 0.31210], [val acc 0.89931]\n------------------------------------------------------------\n*****************************************************\nbest record: [epoch 8], [val loss 0.31210], [val acc 0.89931]\n*****************************************************\n[epoch 9], [iter 100 / 1078], [train loss 0.28584], [train acc 0.89281]\n[epoch 9], [iter 200 / 1078], [train loss 0.28200], [train acc 0.89422]\n[epoch 9], [iter 300 / 1078], [train loss 0.27560], [train acc 0.89750]\n[epoch 9], [iter 400 / 1078], [train loss 0.27571], [train acc 0.89742]\n[epoch 9], [iter 500 / 1078], [train loss 0.27383], [train acc 0.89900]\n[epoch 9], [iter 600 / 1078], [train loss 0.26966], [train acc 0.89943]\n[epoch 9], [iter 700 / 1078], [train loss 0.26619], [train acc 0.90138]\n[epoch 9], [iter 800 / 1078], [train loss 0.26556], [train acc 0.90145]\n[epoch 9], [iter 900 / 1078], [train loss 0.26343], [train acc 0.90205]\n[epoch 9], [iter 1000 / 1078], [train loss 0.26235], [train acc 0.90322]\n","output_type":"stream"},{"name":"stderr","text":"\n0it [00:00, ?it/s]\u001b[A\n1it [00:00,  2.17it/s]\u001b[A\n2it [00:00,  2.17it/s]\u001b[A\n3it [00:01,  2.19it/s]\u001b[A\n4it [00:01,  2.07it/s]\u001b[A\n5it [00:02,  2.10it/s]\u001b[A\n6it [00:02,  2.12it/s]\u001b[A\n7it [00:03,  2.12it/s]\u001b[A\n8it [00:03,  2.15it/s]\u001b[A\n9it [00:04,  2.14it/s]\u001b[A\n10it [00:04,  2.13it/s]\u001b[A\n11it [00:05,  2.16it/s]\u001b[A\n12it [00:05,  2.18it/s]\u001b[A\n13it [00:06,  2.19it/s]\u001b[A\n14it [00:06,  2.17it/s]\u001b[A\n15it [00:06,  2.17it/s]\u001b[A\n16it [00:07,  2.17it/s]\u001b[A\n17it [00:07,  2.17it/s]\u001b[A\n18it [00:08,  2.22it/s]\u001b[A\n 90%|█████████ | 9/10 [1:26:59<09:28, 568.94s/it]","output_type":"stream"},{"name":"stdout","text":"------------------------------------------------------------\n[epoch 9], [val loss 0.42071], [val acc 0.86979]\n------------------------------------------------------------\n[epoch 10], [iter 100 / 1078], [train loss 0.24622], [train acc 0.91500]\n[epoch 10], [iter 200 / 1078], [train loss 0.23084], [train acc 0.91516]\n[epoch 10], [iter 300 / 1078], [train loss 0.23238], [train acc 0.91344]\n[epoch 10], [iter 400 / 1078], [train loss 0.24496], [train acc 0.90867]\n[epoch 10], [iter 500 / 1078], [train loss 0.24173], [train acc 0.91019]\n[epoch 10], [iter 600 / 1078], [train loss 0.24207], [train acc 0.90948]\n[epoch 10], [iter 700 / 1078], [train loss 0.24097], [train acc 0.90929]\n[epoch 10], [iter 800 / 1078], [train loss 0.24159], [train acc 0.90879]\n[epoch 10], [iter 900 / 1078], [train loss 0.24086], [train acc 0.90938]\n[epoch 10], [iter 1000 / 1078], [train loss 0.24208], [train acc 0.90919]\n","output_type":"stream"},{"name":"stderr","text":"\n0it [00:00, ?it/s]\u001b[A\n1it [00:00,  2.17it/s]\u001b[A\n2it [00:00,  2.19it/s]\u001b[A\n3it [00:01,  2.21it/s]\u001b[A\n4it [00:01,  2.18it/s]\u001b[A\n5it [00:02,  2.20it/s]\u001b[A\n6it [00:02,  2.21it/s]\u001b[A\n7it [00:03,  2.19it/s]\u001b[A\n8it [00:03,  2.17it/s]\u001b[A\n9it [00:04,  2.15it/s]\u001b[A\n10it [00:04,  2.17it/s]\u001b[A\n11it [00:05,  2.19it/s]\u001b[A\n12it [00:05,  2.19it/s]\u001b[A\n13it [00:05,  2.19it/s]\u001b[A\n14it [00:06,  2.19it/s]\u001b[A\n15it [00:06,  2.18it/s]\u001b[A\n16it [00:07,  2.18it/s]\u001b[A\n17it [00:07,  2.15it/s]\u001b[A\n18it [00:08,  2.24it/s]\u001b[A\n100%|██████████| 10/10 [1:36:25<00:00, 578.59s/it]","output_type":"stream"},{"name":"stdout","text":"------------------------------------------------------------\n[epoch 10], [val loss 0.36628], [val acc 0.86632]\n------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the model's state dictionary to a file\ntorch.save(model.state_dict(), 'classification_model.pth')","metadata":{"execution":{"iopub.status.busy":"2024-03-15T09:28:30.887513Z","iopub.execute_input":"2024-03-15T09:28:30.888308Z","iopub.status.idle":"2024-03-15T09:28:30.940612Z","shell.execute_reply.started":"2024-03-15T09:28:30.888274Z","shell.execute_reply":"2024-03-15T09:28:30.939887Z"},"trusted":true},"execution_count":46,"outputs":[]}]}